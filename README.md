PRISM Project â€“ Optimizing Inference Efficiency of LLMs
Team Gen AI | Cambridge Institute of Technology
ğŸ“Œ Project Overview
This project aims to optimize the inference efficiency of Large Language Models (LLMs) using multiple performance-enhancing techniques without compromising output quality. We tested and integrated the following methods:
- Quantization (INT8)
- Speculative Decoding (via caching)
- Baseline comparison
- Combined optimization (Quant + Spec)
ğŸ¯ Goal
To build an inference pipeline that:
- Reduces inference time
- Minimizes memory usage
- Provides CLI-based modular control
- Enables integration-ready design for future deployment
ğŸ§ª Models Used
â€¢ distilgpt2 from Hugging Face â†’ https://huggingface.co/distilgpt2
â€¢ Optimized for prompt generation tasks (text output)
ğŸ“‚ Folder Structure
root/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ baseline.py             # Basic LLM inference
â”‚   â”œâ”€â”€ quant_model.py          # Quantized model inference
â”‚   â”œâ”€â”€ spec_decode.py          # Speculative decoding
â”‚   â””â”€â”€ inference.py            # Integrated CLI version
â”œâ”€â”€ diagrams/
â”‚   â”œâ”€â”€ architecture_1.2.png
â”‚   â”œâ”€â”€ architecture_2.1.png
â”‚   â””â”€â”€ architecture_2.4.png
â”œâ”€â”€ benchmark_results.csv       # Timing/memory results
â”œâ”€â”€ demo_video.mp4              # Walkthrough of CLI execution
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ End_Review_PPT.pptx
â”‚   â””â”€â”€ Monthly_Connect_PPTs/
â””â”€â”€ README.md
âš™ï¸ How to Run
Requirements:
â€¢ pip install torch transformers psutil
Scripts:
â€¢ Baseline:
  python code/baseline.py
â€¢ Quantized:
  python code/quant_model.py
â€¢ Speculative Decoding:
  python code/spec_decode.py
â€¢ Combined (Quant + Spec):
  python code/inference.py --both
ğŸ“Š Benchmarked Metrics
Mode	Inference Time (s)	Memory Usage (MB)	Token Count
Baseline	2.04	1690	51
Quant	1.44	1740	50
Spec	1.48	1638	50
Both	0.89	1743	50
ğŸ§  Key Contributions
â€¢ CLI integration of multiple optimization techniques
â€¢ Real-world benchmarking (time/memory/token analysis)
â€¢ Visual dashboards and architecture diagrams
â€¢ Clean, reproducible code structure
ğŸ“ Resources
â€¢ Hugging Face DistilGPT2: https://huggingface.co/distilgpt2
â€¢ Report: docs/End_Review_PPT.pptx
â€¢ Demo video: demo_video.mp4
â€¢ Monthly PPTs: docs/Monthly_Connect_PPTs/
ğŸ§‘â€ğŸ’» Team
â€¢ Gagan R
â€¢ Aaron CIT
â€¢ Sachith CIT
â€¢ Tushar Mishra CIT
ğŸ“… Final Submission Deadline
All materials will be uploaded by May 30, 2025, as per SRI-B guidelines.
